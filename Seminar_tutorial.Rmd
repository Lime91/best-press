---
title: "Best subset variable selection based on Allen's PRESS-Statistic"
author: "Sebastian Sch√ºtz, Konstantin Thiel"
date: "09.07.2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MPV)
library(dplyr)
library(kableExtra)
library(formatR)
library(leaps)
library(stringr)
library(bestglm)
library(MASS)
opts <- options(knitr.kable.NA = "")
```

# Introduction

Within this report variable selection based on **Allen's PRESS-statistic** is presented. Variable selection itself is a popular topic now more than ever. Due to increasing machine resources such variable selection algorithms, which are mostly already existing for a long time, are now possible to execute even on larger scale. The desire to find relations between variables based on algorithms has been present for some time too. It comes from the idea that selection based on algorithms is more objective and therefore in some way better. Another reason may be that one is getting the feeling that no connections were overlooked with such algorithms. It has already been found that variable selection based on algorithms is not automatically the best solution, nevertheless it can help to find a *good* set of variables to describe data. Although the basic procedures like **backward** or **forward elimination** exist for some time, there is still research done to improve or investigate these algorithms.

# Mathematical background

Within this tutorial the **best subset selection**, a very common approach will be applied. This algorithm is often based on criteria like **AIC, BIC, R-squared**, ... Especially for this way of variable selection increasing machine resources are important. The procedure is as follows: 

1. Define ...
  + the criteria on which the selection process is based on.
  + the set of variables selection should be done on.
  + the range of subset sizes you want to find best subsets on (optional).
2. The algorithm finds the *best* subset of variables for each subset size according to the defined criteria. 
 
The selection process shows why the computational effort should not be underestimated. For a working set of $10$ variables, $2^{10}$ models have to be calculated in order to find the *best* according to a given criteria. 

Before referring to **Allen's PRESS-statistic** we want to give a short overview of other common criteria:

* **AIC** (Akaike's information criteria): $AIC = -2l(\hat{\theta}_{ML})+2k$ with $k$ denoting the number of explanatory variables in the model and $l(\hat{\theta}_{ML})$ describing the log-likelihood of the vector $\theta$ which includes $k$ coefficients that result from the maximum-likelihood-estimation. In the case of linear models the AIC can be calculated directly as $AIC = n\cdot ln(\sigma^2)+2k$. [1]
* **BIC** (Bayesian information criteria): $BIC = -2l(\hat{\theta}_{ML})+k\cdot ln(n)$. AIC and BIC differ only regarding the penalty term. While AIC always adds $2k$, BIC considers sample size $n$. It can be concluded that for $n>7$ ($ln(8)>2$), BIC is larger than AIC. [1]


The idea behind these measures is to reward a good model fit and punish for model complexity. The difference to the **PRESS-statistc** is, that a good model fit is assessed different. Instead of maximum-likelihood-estimation, the goodness of prediction for every point in the data set is assessed in the following way: 

$$\begin{aligned}PRESS = \sum_{i=1}^{n}(y_i-\hat{y}_{-i})^2\end{aligned}$$ with $\hat{y_{-i}}$ describing the expected value excluding the $i$-th observation. In other words: For every point in the data set, the goodness of prediction of a model which is based on the remaining $n-1$ data points is calculated (as squared difference between the actual and predicted value). The sum of these predicted errors is then describing the PRESS-statistic which stands for **PRE**diction **S**um of **S**quares. [2]

Although this criterion is not one of the latest findings, there is a connection to a very actual topic: Artificial intelligence. Here, the PRESS-statistic is equivalent to a special case of cross validation, the *leave-one-out* cross validation. The standard procedure follows three steps:

1. Split the data set into $n$ so-called *folds* (*n-fold* cross validation).
2. Combine $n-1$ folds to a training data set, build a model (in AI often a classifier or similar) on it and test it on the $n$-th fold.
3. Do this for all folds and average the error.

Now, leave-one-out cross validation also follows the described steps with the difference that one fold consists of only one data point. This method is also referred to in the literature as the *Jackknife* method. [3]

Especially for larger data sets calculation of PRESS-statistic is cumbersome. According to the description one would need to fit a separate model for every data point in order to estimate the outcome. However, this problem can be circumvented for linear models by using the hat matrix $H=X(X^TX)^{-1}X^T$ to calculate $\hat{y}_{-i}$ in the following way: If $H_{ii}$ denotes the $i$-th diagonal entry of $H$,
$$\begin{aligned}\hat{y}_{-i} = \frac{\hat{y}_{i}-H_{ii}y_i}{1-H_{ii}}\end{aligned}$$ From there it follows, that $$\begin{aligned}PRESS = \sum_{i=1}^n \frac{(y_i-\hat{y}_i)^2}{(1-H_{ii})^2}\end{aligned}$$ That means, that only one linear model has to be built for all $n$ data points. This approach is also used in the following tutorial. A more detailed mathematical explanation can be found here [4][5].

# Implementation in R

## Framework/structure

For implementation in R two packages were used:

* **MPV** for calculation of the PRESS-statistic
* **dplyr** for data preparation

Furthermore three functions were defined:

* *selectPredictors* selects a subset of the explanatory variables based on an integer
```{r, echo=T, eval=T, warning=F, message=F}
#' Extract predictor names from bitcode.
#'
#' @param code An integer in the interval [0, 2^length(predictors) - 1] that 
#' encodes the selected predictors
#' @param predictors A vector of strings with predictor names
selectPredictors <- function(code, predictors) {
  selected <- c()
  for (i in 1:length(predictors)) {
    if (code %% 2 == 1)  # check if current predictor is encoded
      selected <- c(selected, predictors[i])
    code <- code %/% 2  # right shift to obtain next predictor
  }
  return(selected)
}
```
* *computePRESS* calculates the PRESS-statistic for the given explanatory variables (all combinations per subset size)
```{r, echo=T, eval=T, warning=F, message=F}
#' Compute PRESS for a set of predictors specified by the given code
#' 
#' @param code An integer in the interval [0, 2^length(predictors) - 1] that 
#' encodes the selected predictors
#' @param target Name of the target variable in the linear model
#' @param predictors A vector of strings with predictor names
#' @param data Dataframe that contains measurements for the target and all 
#' predictors
computePRESS <- function(code, target, predictors, data) {
  p <- selectPredictors(code, predictors)
  if (is.null(p))  # empty model (iff code == 0)
    p <- "1"
  formula <- paste(target, "~", paste(p, collapse = "+"))
  model <- lm(formula, data)
  return(MPV::PRESS(model))
}
```
* *bestPRESS* selects best model (according to PRESS) per subset size and outputs it as matrix
```{r, echo=T, eval=T, warning=F, message=F, tidy=TRUE, tidy.opts=list(width.cutoff=I(90))}

#' @param target Name of the target variable in the linear model
#' @param predictors A vector of strings with predictor names
#' @param data Dataframe that contains measurements for the target and all 

bestPRESS_df <- function(target, predictors, data) {
  nModels <- 2**length(predictors)
  modelCodes <- seq(0, nModels - 1)
  presses <- sapply(X=modelCodes, FUN=computePRESS, target, predictors, data)
  nrPredictors <- unlist(lapply(sapply(X=modelCodes, FUN=selectPredictors, predictors), FUN = length))
  presses_predictors_df = data.frame(modelCodes, presses, nrPredictors)
  minCode_nrPredictors <- presses_predictors_df %>% arrange(nrPredictors, presses) %>% distinct(nrPredictors, .keep_all = T)
  bestPredictors <- sapply(X=minCode_nrPredictors$modelCodes, FUN = selectPredictors, predictors)
  bestPredictorsdf <- data.frame(do.call(rbind, bestPredictors))
  bestPredictorsdf[upper.tri(bestPredictorsdf)] <- NA
  return(bestPredictorsdf)
}
```

\pagebreak

## Application on real life data

It follows a short demonstration based on a data set on bodyfat.

```{r, echo=F, eval=T, message=F, warning=F, include=T}
data <- read.table("data/case1_bodyfat.txt", header = T, sep = ";")
kable(head(data), format = "latex", booktabs = TRUE) %>%
          kable_styling(latex_options = "scale_down")
```

Applying the algorithm only consists of two steps:

1. Select dependent and independent variables.
2. Start the algorithm with the function *bestPRESS* and pass the selected variables and the data set to the function.

```{r, echo=T, eval=T, message=F, warning=F, tidy=TRUE, tidy.opts=list(width.cutoff=I(90)), include=T}

# dependent variable
target <- "siri"

# compute best subset of predictors for a lm after manual pre-selection
predictors <- c("age", "weight_kg", "height_cm", "neck", "chest", "abdomen",
                "hip", "thigh", "knee", "ankle", "biceps", "forearm", "wrist")

start.time = Sys.time()
best_subsets = bestPRESS_df(target, predictors, data)
end.time = Sys.time()



```

The result shows a matrix consisting the best model according to the PRESS-statistic for each subset size:

```{r, echo=F, eval=T}
kable(best_subsets, format = "latex") %>% kable_styling(latex_options = "scale_down")
```

Although, already using the more efficient calculation of PRESS by use of the hat matrix the execution for a data set with `r dim(data)[1]` rows and `r length(predictors)` explanatory variables takes `r round(end.time - start.time,2)` seconds.

## Application on artificial data

In addition, the algorithm is applied on artificial data as well. The approach for data generation was taken from [6] and slightly modified.

```{r, echo=T, eval=T}

set.seed(1234)
n.sample <- 100
error <- rnorm(n.sample, 0, 0.8)

x1 <- runif(n.sample, -2, 2)
x2 <- runif(n.sample, -1, 4)
x3 <- sample(c(0,1),n.sample, replace=T)
x4 <- 0.8*x1 + rnorm(n.sample, 1, 0.5)
x5 <- 0.4*x1 + rnorm(n.sample, 2, 0.5)
x6 <- -0.5*x1 + rnorm(n.sample, 0, 0.5)
x7 <- rnorm(n.sample, 2, 0.5)
x8 <- 0.9*x2 + rnorm(n.sample, 0, 0.05)
X <- matrix(NA, n.sample, 9)
X[,1] <- x1
X[,2] <- x2
X[,3] <- x3
X[,4] <- x4
X[,5] <- x5
X[,6] <- x6
X[,7] <- x7
X[,8] <- x8
# true model
X[,9] <- 10 + 0.5*x1 - 1*x2 + 3*x3 + error
colnames(X) <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8","y")


fit1 <- lm(y~., data = as.data.frame(X))
summary(fit1)
```

It can be seen, that the selection algorithm is choosing x1,x2 and x3 as *best* explanatory variables which reflects the true model.

```{r, echo=T, eval=T, message=F, warning=F, tidy=TRUE, tidy.opts=list(width.cutoff=I(90)), include=T}

# dependent variable
target_art <- "y"

# compute best subset of predictors for a lm after manual pre-selection
predictors_art <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8")

best_subsets = bestPRESS_df(target_art, predictors_art, as.data.frame(X))
```

```{r, echo=F, eval=T}
kable(best_subsets, format = "latex",col.names=NULL)
```

# Comparison to other algorithms

In order to test the performance of the selection algorithm based on PRESS, a comparison with other criteria within best subset selection is done in this chapter. For comparison the package *leaps* is used. Comparison is done on both data sets.

*Leaps* offers the possibility to find *nbest* models per subset size. Therefore the results can be compared easily for each size. Since information criteria (like AIC, BIC, DIC, ...) differ only in how model complexity is penalized, the *best* models found by the function *regsubsets* do not depend on which information criteria is used (because only models of same sizes are compared).

```{r, echo=T, eval=T, include=T, tidy=TRUE, tidy.opts=list(width.cutoff=I(90))}

form = as.formula(paste(target,"~",paste(predictors, collapse = "+")))
models <- regsubsets(form, data = data, method = "exhaustive", nbest = 1, nvmax = length(predictors))
```


```{r, echo=F, eval=T}
best_models = summary(models)$which[,-1]
for(i in 1:dim(best_models)[2])
{
  best_models[,i]=str_replace(best_models[,i], pattern = "TRUE", colnames(best_models)[i])
  best_models[,i]=str_replace(best_models[,i], pattern = "FALSE", NA_character_)
}
best_models = t(apply(best_models, MARGIN=1, function(x) x[order(match(x,predictors))]))
colnames(best_models) = paste("X",seq(1:dim(best_models)[2]),sep="")

kable(best_models, format = "latex") %>% kable_styling(latex_options = "scale_down")

```



```{r, echo=T, eval=T, include=T, tidy=TRUE, tidy.opts=list(width.cutoff=I(90))}

form = as.formula(paste(target_art,"~",paste(predictors_art, collapse = "+")))
models <- regsubsets(form, data = as.data.frame(X), method = "exhaustive", nbest = 1, nvmax = length(predictors))
```


```{r, echo=F, eval=T}
best_models = summary(models)$which[,-1]
for(i in 1:dim(best_models)[2])
{
  best_models[,i]=str_replace(best_models[,i], pattern = "TRUE", colnames(best_models)[i])
  best_models[,i]=str_replace(best_models[,i], pattern = "FALSE", NA_character_)
}
best_models = t(apply(best_models, MARGIN=1, function(x) x[order(match(x,predictors_art))]))
colnames(best_models) = paste("X",seq(1:dim(best_models)[2]),sep="")

kable(best_models, format = "latex",col.names=NULL)

```

# Conclusion

The whole algorithm for using PRESS as criteria is implemented by three functions, where only the *bestPRESS* function is needed for execution. This guarantees an easy handling for all sorts of applications. The user only has to provide the data set (*data*) along with the information which variables are explanatory (*predictors*) and which is the dependent one (*target*). 

For the bodyfat data set it could be shown that overall similar variables were selected in comparison to best subset selection with AIC/BIC/... Only 3/13 models were different. However, it cannot be validated how well variable selection performance was in general on this data set because no information about the true model was available. 

The results for the artificial data set were even more similar to each other. Only 1/8 models were different between PRESS and AIC/BIC/... The increase on similarity could also be attributed to the rather low complexity of the data set.

The main goal of this tutorial was to provide a working **best subset selection** algorithm with **PRESS** as selection criteria, which was done. For more detailed comparison with other selection algorithms further investigations would be needed.

# Limitations

* Interaction terms

It has not yet been discussed how to deal with interaction terms. Although it needs some additional preparation, it is possible to add interactions within the *predictors*-vector by just combining the variable names with **:** as separator. For few interaction terms this can be done by hand. At some point *paste*-function can be useful to adapt the *predictors*-vector.

```{r, echo=T,eval=T,include=T,warning=F,message=F}
# example
predictors <- c("age", "weight_kg", "height_cm", "neck", "chest", "abdomen",
                "hip", "thigh", "knee", "ankle", "biceps", "forearm", "wrist","age:knee")

best_subsets = bestPRESS_df(target, predictors, data)

```

```{r, echo=F, eval=T}
kable(best_subsets, format = "latex") %>% kable_styling(latex_options = "scale_down")
```

* Algorithm runtime

Variable selection is often performed in larger data sets. It was shown that with 13 explanatory variables the algorithm already takes some time. Since the number of models increases exponentially, it has to be checked if the usage of the algorithm is still feasible at some point.

# Sources

1. Sachs, L., & Hedderich, J. (2006). Angewandte Statistik: Methodensammlung mit R. Springer-Verlag.
2. Allen, D. M. (1974). The relationship between variable selection and data agumentation and a method for prediction. technometrics, 16(1), 125-127.
3. Kruse, R., Borgelt, C., Braune, C., Mostaghim, S., Steinbrecher, M., Klawonn, F., & Moewes, C. (2011). Computational intelligence. Vieweg+ Teubner Verlag.
4. https://statisticaloddsandends.wordpress.com/2018/07/30/the-press-statistic-for-linear-regression/
5. http://statweb.stanford.edu/~owen/courses/305a/305MinNotesMarked.pdf
6. https://www.rpubs.com/lictof/480764
